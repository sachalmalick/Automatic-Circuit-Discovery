{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model sachalmalick/gpt2-transprop-ft-welterweight into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from acdc.transprop.utils import get_finetuned_gpt2\n",
    "\n",
    "model = get_finetuned_gpt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model sachalmalick/gpt2-transprop-ft-welterweight into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "torch.Size([10, 50257])\n"
     ]
    }
   ],
   "source": [
    "import acdc.transprop.utils as tp\n",
    "import importlib\n",
    "importlib.reload(tp)\n",
    "transprop_things = tp.get_transprop_things(20, \"frac_correct\")\n",
    "data = tp.get_prompt_data(5, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50257])\n",
      "tensor([31853,  5657,   540,   257, 20690], device='cuda:0')\n",
      "[' barric', 'bar', 'able', ' a', ' siege']\n",
      "banana implies comb and if comb then barricade therefore by the transitive property banana also implies<|endoftext|><|endoftext|>\n",
      "tensor(31853, device='cuda:0')\n",
      "torch.Size([10, 21, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([29.1176, 18.2405, 25.2618, 12.1479, 31.6272, 15.0748, 25.9485, 12.0949,\n",
      "        42.0337, 26.0337], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([ 3.4341, -6.2562,  1.2916, -4.8504, -2.1465,  0.6659, 10.6417, -6.2371,\n",
      "         3.0601,  5.4880], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([True, True, True, True, True, True, True, True, True, True],\n",
      "       device='cuda:0')\n",
      "tensor(-1., device='cuda:0')\n",
      "tensor(31853, device='cuda:0')\n",
      " barric\n",
      "Cross entropy loss tensor(1.2521, device='cuda:0', grad_fn=<NllLossBackward0>) tensor(1.2521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Next word prediction: [' edible', 'ible', 'ed', 'able', ' usable']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ds = transprop_things.validation_data\n",
    "model = transprop_things.tl_model\n",
    "output = transprop_things.tl_model(ds)\n",
    "indices = torch.sum(transprop_things.validation_mask, dim=1)\n",
    "logits = torch.stack([\n",
    "    torch.squeeze(output[i, indices[i] - 1, :]) for i in range(indices.shape[0])\n",
    "])\n",
    "print(logits.shape)\n",
    "top_indices = torch.topk(logits, k=5, dim=-1).indices[9]\n",
    "print(top_indices)\n",
    "predicted_words = [model.tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "print(predicted_words)\n",
    "print(model.tokenizer.decode(ds[9]))\n",
    "print(transprop_things.validation_labels[9])\n",
    "print(transprop_things.validation_metric(output))\n",
    "print(transprop_things.validation_labels[9])\n",
    "print(model.tokenizer.decode(transprop_things.validation_labels[9]))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "loss = loss_fn(logits, transprop_things.validation_labels)\n",
    "wrong_loss = loss_fn(logits, transprop_things.validation_labels)\n",
    "print(\"Cross entropy loss\", loss, wrong_loss)\n",
    "# # Define a prompt\n",
    "prompt = \"If all apples are fruits and all fruits are edible, then it can be inferred that all apples are\"\n",
    "\n",
    "# # Tokenize the prompt\n",
    "input_ids = model.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate the next word\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs[:, -1, :]  # Take the logits of the last position\n",
    "    top_indices = torch.topk(logits, k=5, dim=-1).indices[0]\n",
    "\n",
    "\n",
    "# # Decode the predicted index to get the next word\n",
    "predicted_words = [model.tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "\n",
    "print(\"Next word prediction:\", predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acdc.transprop.utils as tp\n",
    "import importlib\n",
    "importlib.reload(tp)\n",
    "from acdc.ioi.utils import get_gpt2_small\n",
    "import gc\n",
    "def perform_baseline_evaluation(metrics, model, prompts):\n",
    "    results = {}\n",
    "    for prompt in prompts:\n",
    "        metric_results = {}\n",
    "        for metric in metrics:\n",
    "            transprop_things = tp.get_transprop_things(20,\n",
    "                                                    metric,    \n",
    "                                                    model=model,\n",
    "                                                    prompt=prompt)\n",
    "            ds = transprop_things.validation_data\n",
    "            output = transprop_things.tl_model(ds)\n",
    "            metric_result = transprop_things.validation_metric(output)\n",
    "            metric_results[metric] = metric_result\n",
    "            model.reset_hooks()\n",
    "        results[prompt] = metric_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = get_gpt2_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 50257])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 21.99 GiB total capacity; 21.13 GiB already allocated; 1024.00 KiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_templates \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{a}\u001b[39;00m\u001b[38;5;124m implies \u001b[39m\u001b[38;5;132;01m{b}\u001b[39;00m\u001b[38;5;124m and if \u001b[39m\u001b[38;5;132;01m{b}\u001b[39;00m\u001b[38;5;124m then \u001b[39m\u001b[38;5;132;01m{c}\u001b[39;00m\u001b[38;5;124m therefore by the transitive property \u001b[39m\u001b[38;5;132;01m{a}\u001b[39;00m\u001b[38;5;124m also implies\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif all \u001b[39m\u001b[38;5;132;01m{a}\u001b[39;00m\u001b[38;5;124m are \u001b[39m\u001b[38;5;132;01m{b}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{b}\u001b[39;00m\u001b[38;5;124m are \u001b[39m\u001b[38;5;132;01m{c}\u001b[39;00m\u001b[38;5;124m then all \u001b[39m\u001b[38;5;132;01m{a}\u001b[39;00m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{a}\u001b[39;00m\u001b[38;5;124m is a type of \u001b[39m\u001b[38;5;132;01m{b}\u001b[39;00m\u001b[38;5;124m and all \u001b[39m\u001b[38;5;132;01m{b}\u001b[39;00m\u001b[38;5;124m are \u001b[39m\u001b[38;5;132;01m{c}\u001b[39;00m\u001b[38;5;124m therefore \u001b[39m\u001b[38;5;132;01m{a}\u001b[39;00m\u001b[38;5;124m is also a type of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m ]\n\u001b[0;32m---> 10\u001b[0m baseline \u001b[38;5;241m=\u001b[39m \u001b[43mperform_baseline_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkl_div\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_templates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(baseline)\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mperform_baseline_evaluation\u001b[0;34m(metrics, model, prompts)\u001b[0m\n\u001b[1;32m     11\u001b[0m transprop_things \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mget_transprop_things(\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     12\u001b[0m                                         metric,    \n\u001b[1;32m     13\u001b[0m                                         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     14\u001b[0m                                         prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m     15\u001b[0m ds \u001b[38;5;241m=\u001b[39m transprop_things\u001b[38;5;241m.\u001b[39mvalidation_data\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtransprop_things\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m metric_result \u001b[38;5;241m=\u001b[39m transprop_things\u001b[38;5;241m.\u001b[39mvalidation_metric(output)\n\u001b[1;32m     18\u001b[0m metric_results[metric] \u001b[38;5;241m=\u001b[39m metric_result\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acdc-2p4DFdz0-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acdc-2p4DFdz0-py3.10/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:405\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, stop_at_layer, past_kv_cache, past_left_attention_mask)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    402\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    403\u001b[0m         )\n\u001b[0;32m--> 405\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acdc-2p4DFdz0-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acdc-2p4DFdz0-py3.10/lib/python3.10/site-packages/transformer_lens/components.py:1044\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, left_attention_mask)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m add_head_dimension(shortformer_pos_embed)\n\u001b[1;32m   1039\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[0;32m-> 1044\u001b[0m         query_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[1;32m   1046\u001b[0m         key_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(key_input)\n\u001b[1;32m   1047\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m shortformer_pos_embed),\n\u001b[1;32m   1048\u001b[0m         value_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(value_input),\n\u001b[1;32m   1049\u001b[0m         past_kv_cache_entry\u001b[38;5;241m=\u001b[39mpast_kv_cache_entry,\n\u001b[1;32m   1050\u001b[0m         left_attention_mask\u001b[38;5;241m=\u001b[39mleft_attention_mask,\n\u001b[1;32m   1051\u001b[0m     )\n\u001b[1;32m   1052\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1054\u001b[0m     resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_mid(\n\u001b[1;32m   1055\u001b[0m         resid_pre \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m   1056\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acdc-2p4DFdz0-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/acdc-2p4DFdz0-py3.10/lib/python3.10/site-packages/transformer_lens/components.py:275\u001b[0m, in \u001b[0;36mLayerNormPre.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# [batch, pos, length]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m scale: Union[\n\u001b[1;32m    273\u001b[0m     Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos 1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    274\u001b[0m     Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos head_index 1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m--> 275\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_scale((\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\u001b[38;5;241m.\u001b[39msqrt())\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_normalized(x \u001b[38;5;241m/\u001b[39m scale)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 21.99 GiB total capacity; 21.13 GiB already allocated; 1024.00 KiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "prompt_templates = [\n",
    "    \"{a} implies {b} and if {b} then {c} therefore by the transitive property {a} also implies\",\n",
    "    \"if all {a} are {b} and {b} are {c} then all {a} are\",\n",
    "    \"if all {a} are {b} and {b} is a type of {c} then it can be inferred that all {a} are a type of\",\n",
    "    \"{a} implies {b} and if {b} then {c} therefore {a} also implies\",\n",
    "    \"{a} implies {b} and if {b} then {c} therefore by the transitive property {a} also implies\",\n",
    "    \"{a} implies {b} and {b} implies {c} then by the transitive property {a} also implies\",\n",
    "    \"{a} is a type of {b} and all {b} are {c} therefore {a} is also a type of\"\n",
    "]\n",
    "baseline = perform_baseline_evaluation([\"kl_div\"], model, prompt_templates)\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
