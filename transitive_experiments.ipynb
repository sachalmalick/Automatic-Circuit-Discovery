{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from acdc.ioi.utils import get_gpt2_small\n",
    "\n",
    "model = get_gpt2_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "torch.Size([10, 50257])\n"
     ]
    }
   ],
   "source": [
    "import acdc.transprop.utils as tp\n",
    "import importlib\n",
    "importlib.reload(tp)\n",
    "transprop_things = tp.get_transprop_things(20, \"frac_correct\")\n",
    "data = tp.get_prompt_data(5, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50257])\n",
      "tensor([31853,  1974,   262,   257,   275], device='cuda:0')\n",
      "[' barric', ' comb', ' the', ' a', ' b']\n",
      "banana implies comb and if comb then barricade therefore by the transitive property banana also implies<|endoftext|><|endoftext|>\n",
      "tensor(31853, device='cuda:0')\n",
      "torch.Size([10, 21, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([-2.5023, -7.2054, -0.3910, -1.9623, -2.8029, -3.7070,  0.1475, -2.6524,\n",
      "        12.7613, -3.7197], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([-3.1773, -2.0016, -3.9363, -2.1811, -4.2002, -1.3575,  0.1300,  0.5678,\n",
      "        16.3911, -1.0792], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor(-0.5000, device='cuda:0')\n",
      "tensor(31853, device='cuda:0')\n",
      " barric\n",
      "Cross entropy loss tensor(3.4708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Next word prediction: [' edible', ' fruits', ' fruit', ' not', ' the']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ds = transprop_things.validation_data\n",
    "model = transprop_things.tl_model\n",
    "output = transprop_things.tl_model(ds)\n",
    "indices = torch.sum(transprop_things.validation_mask, dim=1)\n",
    "logits = torch.stack([\n",
    "    torch.squeeze(output[i, indices[i] - 1, :]) for i in range(indices.shape[0])\n",
    "])\n",
    "print(logits.shape)\n",
    "top_indices = torch.topk(logits, k=5, dim=-1).indices[9]\n",
    "print(top_indices)\n",
    "predicted_words = [model.tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "print(predicted_words)\n",
    "print(model.tokenizer.decode(ds[9]))\n",
    "print(transprop_things.validation_labels[9])\n",
    "print(transprop_things.validation_metric(output))\n",
    "print(transprop_things.validation_labels[9])\n",
    "print(model.tokenizer.decode(transprop_things.validation_labels[9]))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "loss = loss_fn(logits, transprop_things.validation_labels)\n",
    "wrong_loss = loss_fn(logits, transprop_things.validation_labels)\n",
    "print(\"Cross entropy loss\", loss, wrong_loss)\n",
    "# # Define a prompt\n",
    "prompt = \"If all apples are fruits and all fruits are edible, then it can be inferred that all apples are\"\n",
    "\n",
    "# # Tokenize the prompt\n",
    "input_ids = model.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate the next word\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs[:, -1, :]  # Take the logits of the last position\n",
    "    top_indices = torch.topk(logits, k=5, dim=-1).indices[0]\n",
    "\n",
    "\n",
    "# # Decode the predicted index to get the next word\n",
    "predicted_words = [model.tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "\n",
    "print(\"Next word prediction:\", predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acdc.transprop.utils as tp\n",
    "import importlib\n",
    "importlib.reload(tp)\n",
    "from acdc.ioi.utils import get_gpt2_small\n",
    "import gc\n",
    "def perform_baseline_evaluation(metrics, model, prompts):\n",
    "    results = {}\n",
    "    for prompt in prompts:\n",
    "        metric_results = {}\n",
    "        for metric in metrics:\n",
    "            transprop_things = tp.get_transprop_things(20,\n",
    "                                                    metric,    \n",
    "                                                    model=model,\n",
    "                                                    prompt=prompt)\n",
    "            ds = transprop_things.validation_data\n",
    "            output = transprop_things.tl_model(ds)\n",
    "            metric_result = transprop_things.validation_metric(output)\n",
    "            metric_results[metric] = metric_result\n",
    "            model.reset_hooks()\n",
    "        results[prompt] = metric_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = get_gpt2_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 21, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 21, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([-2.5023, -7.2054, -0.3910, -1.9623, -2.8029, -3.7070,  0.1475, -2.6524,\n",
      "        12.7613, -3.7197], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([-3.1773, -2.0016, -3.9363, -2.1811, -4.2002, -1.3575,  0.1300,  0.5678,\n",
      "        16.3911, -1.0792], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 17, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 17, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([-2.6249e+00, -7.3407e+00, -4.1402e-01, -2.0514e+00, -2.9313e+00,\n",
      "        -3.7960e+00, -3.7928e-03, -2.7974e+00, -2.7686e+00, -3.8357e+00],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([-3.2544, -2.1323, -3.9307, -2.2850, -4.2473, -1.3740, -0.0423,  0.4165,\n",
      "        -1.6351, -1.2688], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 28, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 28, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([-2.5566, -7.2266, -0.3529, -1.9403, -2.9349, -3.7053,  0.0176, -2.7707,\n",
      "        -2.6655, -3.8306], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([-3.2067, -2.0312, -3.8810, -2.1298, -4.1481, -1.2834,  0.0058,  0.5030,\n",
      "        -1.5710, -1.2388], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 16, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 16, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([-2.5731, -7.3528, -0.4661, -2.0523, -2.9057, -3.8231,  0.1351, -2.7275,\n",
      "        12.5894, -3.7656], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([-3.2936, -2.0768, -4.0217, -2.2578, -4.2374, -1.3858,  0.0519,  0.5345,\n",
      "        15.3922, -1.1089], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 21, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 21, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([-2.5023, -7.2054, -0.3910, -1.9623, -2.8029, -3.7070,  0.1475, -2.6524,\n",
      "        12.7613, -3.7197], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([-3.1773, -2.0016, -3.9363, -2.1811, -4.2002, -1.3575,  0.1300,  0.5678,\n",
      "        16.3911, -1.0792], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 20, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 20, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([-2.5439, -7.2179, -0.4369, -1.9757, -2.8330, -3.7376,  0.1073, -2.6881,\n",
      "        11.8742, -3.7137], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([-3.2267, -2.0128, -3.9859, -2.2370, -4.1843, -1.3713,  0.0944,  0.5142,\n",
      "        15.7787, -1.0878], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 22, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 50257])\n",
      "torch.Size([10, 22, 50257])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "tensor([-2.5111, -7.2264, -0.4489, -1.9222, -2.8444, -3.7694,  0.0165, -2.7757,\n",
      "        11.7895, -3.7226], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "tensor([-3.3409, -2.0454, -4.0206, -2.1683, -4.1874, -1.3249, -0.0225,  0.5115,\n",
      "        14.5136, -1.1674], device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "torch.Size([10, 50257])\n",
      "{'{a} implies {b} and if {b} then {c} therefore by the transitive property {a} also implies': {'logit_diff': tensor(1.1190, device='cuda:0', grad_fn=<NegBackward0>), 'frac_correct': tensor(-0.5000, device='cuda:0'), 'kl_div': tensor(13.3388, device='cuda:0', grad_fn=<MeanBackward0>)}, 'if all {a} are {b} and {b} are {c} then all {a} are': {'logit_diff': tensor(0.8810, device='cuda:0', grad_fn=<NegBackward0>), 'frac_correct': tensor(-0.5000, device='cuda:0'), 'kl_div': tensor(14.6679, device='cuda:0', grad_fn=<MeanBackward0>)}, 'if all {a} are {b} and {b} is a type of {c} then it can be inferred that all {a} are a type of': {'logit_diff': tensor(0.8985, device='cuda:0', grad_fn=<NegBackward0>), 'frac_correct': tensor(-0.5000, device='cuda:0'), 'kl_div': tensor(18.1103, device='cuda:0', grad_fn=<MeanBackward0>)}, '{a} implies {b} and if {b} then {c} therefore {a} also implies': {'logit_diff': tensor(1.0539, device='cuda:0', grad_fn=<NegBackward0>), 'frac_correct': tensor(-0.5000, device='cuda:0'), 'kl_div': tensor(13.3893, device='cuda:0', grad_fn=<MeanBackward0>)}, '{a} implies {b} and {b} implies {c} then by the transitive property {a} also implies': {'logit_diff': tensor(1.1447, device='cuda:0', grad_fn=<NegBackward0>), 'frac_correct': tensor(-0.5000, device='cuda:0'), 'kl_div': tensor(12.8941, device='cuda:0', grad_fn=<MeanBackward0>)}, '{a} is a type of {b} and all {b} are {c} therefore {a} is also a type of': {'logit_diff': tensor(1.0162, device='cuda:0', grad_fn=<NegBackward0>), 'frac_correct': tensor(-0.5000, device='cuda:0'), 'kl_div': tensor(13.9347, device='cuda:0', grad_fn=<MeanBackward0>)}}\n"
     ]
    }
   ],
   "source": [
    "prompt_templates = [\n",
    "    \"{a} implies {b} and if {b} then {c} therefore by the transitive property {a} also implies\",\n",
    "    \"if all {a} are {b} and {b} are {c} then all {a} are\",\n",
    "    \"if all {a} are {b} and {b} is a type of {c} then it can be inferred that all {a} are a type of\",\n",
    "    \"{a} implies {b} and if {b} then {c} therefore {a} also implies\",\n",
    "    \"{a} implies {b} and if {b} then {c} therefore by the transitive property {a} also implies\",\n",
    "    \"{a} implies {b} and {b} implies {c} then by the transitive property {a} also implies\",\n",
    "    \"{a} is a type of {b} and all {b} are {c} therefore {a} is also a type of\"\n",
    "]\n",
    "baseline = perform_baseline_evaluation([\"logit_diff\", \"frac_correct\", \"kl_div\"], model, prompt_templates)\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
